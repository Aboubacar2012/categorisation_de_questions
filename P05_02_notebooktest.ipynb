{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modelisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from gensim.utils import ClippedCorpus\n",
    "import gensim\n",
    "import tqdm\n",
    "from gensim.models import Phrases\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import LdaModel, CoherenceModel, LdaMulticore, TfidfModel, Nmf\n",
    "from nlp_module import remove_stopwords, plot_top_words\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>415192</td>\n",
       "      <td>good way create simple python web service</td>\n",
       "      <td>I use python year I little experience python w...</td>\n",
       "      <td>&lt;python&gt;&lt;web-services&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>415344</td>\n",
       "      <td>log implementation prefer</td>\n",
       "      <td>I implement log class c try decide I curious k...</td>\n",
       "      <td>&lt;debugging&gt;&lt;language-agnostic&gt;&lt;logging&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>414981</td>\n",
       "      <td>directly modify list element</td>\n",
       "      <td>I struct struct map public int size public map...</td>\n",
       "      <td>&lt;c#&gt;&lt;.net&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                                      Title  \\\n",
       "0  415192  good way create simple python web service   \n",
       "1  415344                  log implementation prefer   \n",
       "2  414981               directly modify list element   \n",
       "\n",
       "                                                Body  \\\n",
       "0  I use python year I little experience python w...   \n",
       "1  I implement log class c try decide I curious k...   \n",
       "2  I struct struct map public int size public map...   \n",
       "\n",
       "                                      Tags  \n",
       "0                   <python><web-services>  \n",
       "1  <debugging><language-agnostic><logging>  \n",
       "2                               <c#><.net>  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets/posts_clean.csv\")\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove < and > around Tags\n",
    "data[\"Tags\"] = data[\"Tags\"].replace({\"<\" : \" \"}, regex=True)\n",
    "data[\"Tags\"] = data[\"Tags\"].replace({\">\" : \" \"}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Body\"] = data[\"Body\"].str.lower()\n",
    "data[\"Body\"] = data[\"Body\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>415192</td>\n",
       "      <td>good way create simple python web service</td>\n",
       "      <td>use python year little experience python web p...</td>\n",
       "      <td>python  web-services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>415344</td>\n",
       "      <td>log implementation prefer</td>\n",
       "      <td>implement log class c try decide curious know ...</td>\n",
       "      <td>debugging  language-agnostic  logging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>414981</td>\n",
       "      <td>directly modify list element</td>\n",
       "      <td>struct struct map public int size public map i...</td>\n",
       "      <td>c#  .net</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id                                      Title  \\\n",
       "0  415192  good way create simple python web service   \n",
       "1  415344                  log implementation prefer   \n",
       "2  414981               directly modify list element   \n",
       "\n",
       "                                                Body  \\\n",
       "0  use python year little experience python web p...   \n",
       "1  implement log class c try decide curious know ...   \n",
       "2  struct struct map public int size public map i...   \n",
       "\n",
       "                                      Tags  \n",
       "0                    python  web-services   \n",
       "1   debugging  language-agnostic  logging   \n",
       "2                                c#  .net   "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = data[\"Body\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "# Create a dictionary representation of the documents.\n",
    "dct = Dictionary(docs)\n",
    "dct.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dct.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 25556\n",
      "Number of documents: 164598\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dct))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters.\n",
    "chunksize = 2000\n",
    "passes = 10\n",
    "iterations = 200\n",
    "eval_every = None\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dct[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dct.id2token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Non Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "tfidf = tfidf_vectorizer.fit_transform(data[\"Body\"])\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_nmf = tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.09108897, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.bool_' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_40044/100396349.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopic_nums\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     nmf = Nmf(corpus=corpus_nmf.all(),\n\u001b[0m\u001b[0;32m     10\u001b[0m               \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m               \u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\nmf.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, chunksize, passes, kappa, minimum_probability, w_max_iter, w_stop_condition, h_max_iter, h_stop_condition, eval_every, normalize, random_state)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\nmf.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, corpus, chunksize, passes, eval_every)\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[0mgrouper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mchunk_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    620\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsc_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m                     \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mchunkize_serial\u001b[1;34m(iterable, chunksize, as_numpy, dtype)\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m     \"\"\"\n\u001b[1;32m-> 1236\u001b[1;33m     \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1237\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mas_numpy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.bool_' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Create a list of the topic numbers we want to try\n",
    "topic_nums = list(np.arange(2, 10 + 1, 1))\n",
    "\n",
    "# Run the nmf model and calculate the coherence score\n",
    "# for each number of topics\n",
    "coherence_scores = []\n",
    "\n",
    "for num in topic_nums:\n",
    "    nmf = Nmf(corpus=corpus_nmf,\n",
    "              num_topics=num,\n",
    "              id2word=id2word,\n",
    "              chunksize=chunksize,\n",
    "              passes=passes,\n",
    "              kappa=.1,\n",
    "              minimum_probability=0.01,\n",
    "              w_max_iter=300,\n",
    "              w_stop_condition=0.0001,\n",
    "              h_max_iter=100,\n",
    "              h_stop_condition=0.001,\n",
    "              eval_every=eval_every,\n",
    "              normalize=True,\n",
    "              random_state=42)\n",
    "    \n",
    "# Run the coherence model to get the score\n",
    "    cm = CoherenceModel(\n",
    "        model=nmf,\n",
    "        texts=docs,\n",
    "        dictionary=dct,\n",
    "        coherence='c_v')\n",
    "    \n",
    "    coherence_scores.append(round(cm.get_coherence(), 5))\n",
    "\n",
    "# Get the number of topics with the highest coherence score\n",
    "scores = list(zip(topic_nums, coherence_scores))\n",
    "best_num_topics = sorted(scores, key=lambda elem: elem[1], reverse=True)[0][0]\n",
    "\n",
    "print(\"Best number of topics:\", best_num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = Nmf(corpus,\n",
    "          id2word = id2word,\n",
    "          chunksize = 2000,\n",
    "          num_topics=best_num_topics,\n",
    "          kappa=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_nmf = CoherenceModel(model=nmf,\n",
    "                                     texts=docs,\n",
    "                                     dictionary=dct,\n",
    "                                     coherence='c_v')\n",
    "coherence_nmf = coherence_model_nmf.get_coherence()\n",
    "print('Coherence Score: ', coherence_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics=best_num_topics\n",
    "n_top_words = 30\n",
    "\n",
    "nmf = NMF(n_components=n_topics, \n",
    "          random_state=42, alpha=.1, \n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "nmf_embedding = nmf.transform(tfidf)\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "print(\"Topics found via NMF:\")\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(\"\\nTopic {}:\".format(topic_idx))\n",
    "    print(\" \".join(['[{}]'.format(feature_names[i]) for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_words(nmf, tfidf_feature_names, n_top_words, \"Topics in NMF model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the topic numbers we want to try\n",
    "topic_nums = list(np.arange(2, 10 + 1, 1))\n",
    "\n",
    "# Run the nmf model and calculate the coherence score\n",
    "# for each number of topics\n",
    "coherence_scores = []\n",
    "\n",
    "for num in topic_nums:\n",
    "    lda = LdaModel(corpus=corpus,\n",
    "                         id2word=id2word,\n",
    "                         num_topics=num, \n",
    "                         random_state=42,\n",
    "                         chunksize=chunksize,\n",
    "                         passes=passes,\n",
    "                         iterations=iterations,\n",
    "                         alpha=\"auto\",\n",
    "                         eta=\"auto\",\n",
    "                         eval_every=eval_every)\n",
    "    \n",
    "# Run the coherence model to get the score\n",
    "    cm = CoherenceModel(\n",
    "        model=lda,\n",
    "        texts=docs,\n",
    "        dictionary=dct,\n",
    "        coherence='c_v')\n",
    "    \n",
    "    coherence_scores.append(round(cm.get_coherence(), 5))\n",
    "\n",
    "# Get the number of topics with the highest coherence score\n",
    "scores = list(zip(topic_nums, coherence_scores))\n",
    "best_num_topics = sorted(scores, key=lambda elem: elem[1], reverse=True)[0][0]\n",
    "\n",
    "print(\"Best number of topics:\", best_num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(corpus=corpus,\n",
    "                     id2word=id2word,\n",
    "                     num_topics=best_num_topics, \n",
    "                     random_state=42,\n",
    "                     chunksize=chunksize,\n",
    "                     passes=passes,\n",
    "                     iterations=iterations,\n",
    "                     alpha=\"auto\",\n",
    "                     eta=\"auto\",\n",
    "                     eval_every=eval_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda = CoherenceModel(model=lda, texts=docs, dictionary=dct, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "lda_vis = gensimvis.prepare(lda, corpus, dct)\n",
    "lda_vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Supervised learning for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X =\n",
    "y = \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=.75, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\"kernel\" : [\"linear\", \"rbf\", \"poly\", \"sigmoid\"],\n",
    "          \"decision_function_shape\" : [\"ovr\", \"ovo\"]}\n",
    "\n",
    "grid_search = GridSearchCV(svm_clf, \n",
    "                           param_grid=params, \n",
    "                           scoring=\"accuracy\",\n",
    "                           cv=5,\n",
    "                           n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_clf = MultinomialNB(random_state=42)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db691860837755bfd223dbb19bf59c90fd58faf4b85973e04a456a30cef6b76b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
